% !TEX root = ./abstract.tex
% \paragraph{Category Taxonomy}
% Multi-class classification with a targeted taxonomy structure among categories is often considered as hierarchical classification. In this case the targets are often understood as sets of categories with closely related categories grouped in the same set. 

% Knowledge sharing (cite) has been shown to be important in multi-task learning scenarios like this where tasks are closely related and knowledge gained from one task can be used to facilitate the training of another one. 
% We will also see that knowledge sharing is now possible by solving a unified single problem whereas before each subproblem is trained independently. 

\begin{figure}
\label{fig:taxonomy}
\includegraphics[width=0.5\textwidth]{resources/circular_tree.pdf}
\caption{An illustration of in-house taxonomy.}
\end{figure}

\paragraph{Negative Sampling With Log Loss}
A standard approach to deal with classes more than binary is to maximize the log probability estimated using softmax. This can become computationally expensive when the output space is large due to the need to compute the normalization factor. In this work we make use of a more efficient alternative based on negative sampling \cite{mikolov2013distributed},
\begin{align}
\label{equ:obj}
    \JCal(\omega) = 
    - \log \sigma(f_{x, c_x}(\omega)) 
    - \sum_{i=1}^k E_{c_i \sim p_n(c)} \log \sigma(-f_{x, c_i}(\omega))
\end{align}
Here, the logit function $f_{x, c}(\omega)$ is parameterized by $\omega$ and applying the sigmoid function $\sigma(f_{x, c}(\omega))$ approximates the probability of the item $x \in \XCal$ being in the category $c \in \CCal$. 
By minimizing \eqref{equ:obj} we increase the probability of $x$ being in the ground truth category $c_x$ while decreasing the probability of $x$ in the wrong category sampled from the noise distribution $p_n(c)$. In this work we approximate it using $n_c / n$ which is the number of category $c$ in the training set divided by the total size of the training set. Note that while \eqref{equ:obj} does not constitute an approximation to log probability of the full softmax, it serves the purpose of classification where we only care about the top scores.

\paragraph{Joint Training of Item and Category Vector}
Here we discuss the specific structure we use for the logit function $f_{x, c}(\omega)$. The trainable parameters consist of the item matrix $U$ and a category matrix $V$. Each row in the item matrix corresponds to a feature we extract from the item description, and each row in the category matrix represents a particular category. The logits for all categories can then be obtained as,
\begin{align*}
    f_{x, \CCal}(U, V) = x^TUV^T
\end{align*}
Note that if letting $W = UV^T$ the above equation becomes $x^TW$ which is a standard form of multi-class classifier. By splitting $W$ into two matrices $U$ and $V$ we are essentially imposing a low-rank structure onto the classifier. This greatly reduces the number of learning parameters and has been widely used in recommendation system, multi-task learning, etc, to improve generalization.

In feature extraction we include both unigram and n-gram. We use all unigram and apply hashing tricks \cite{weinberger2009feature} to n-gram, e.g., hashing each n-gram to a preset number of buckets. This has been shown to avoid overfitting and to improve system efficiency. In other words, if there are 1000 unique tokens and the bucket size is set to 500, then the dimension of $x$ will be 1000 + 500 = 1500 where each entry is 0 or 1 denoting whether the corresponding token or n-gram is present in the item description or not. In practice the size of the vector $x$ will be millions but very sparse. For efficient implementation we only store the indices of the non-zeros entries in $x$ and the matrix-vector multiplication between $x$ and $U$ takes only a loop through the non-zeros entries which are only the number of tokens in the item description. 

Now let us look at how we enable knowledge sharing in this context. Specifically, each task, or category in this case, corresponds to one row of the category matrix $V$, which captures the uniqueness of the task. The common knowledge shared by all task is reflected in the item matrix $U$. During training $U$ will be updated by examples regardless of which categories they belong to, while the rows in $V$ are only updated with the examples in the corresponding categories. In this way $U$ can help improve the accuracy of the category even though there are only a few training data in that category. This addresses the aforementioned \emph{data scarcity and imbalance} issues in the top-down approach.

\paragraph{Asynchronous SGD with Momentum}
The embedding matrices $U, V$ are trained using stochastic gradient descent with momentum and the updates are applied asynchronously in multiple threads. 
The major procedures are presented in Algorithm \ref{alg:asgd}. 
The main update rules are described in Line \ref{stp:asgd-update-begin}-\ref{stp:asgd-update-end}. 
Here $v_{c_x}$ denotes the vector representation of category $c_x$ and $u_i, i \in nnz(x)$ denotes the feature vector corresponding to the $i$th non-zero entry of $x$.
We will see that when the training dataset is highly sparse Algorithm \ref{alg:asgd} is the same as if the operations were executed in some sequential order of block coordinate descent. In fact, consider two training examples $(x_1, c_1)$ and $(x_2, c_2)$ such that $nnz(x_1) \cap nnz(x_2) = \emptyset$ and that $c_1 \neq c_2$. According to Algorithm \ref{alg:asgd} $(x_1, c_1)$ and $(x_2, c_2)$ will result in update on $\omega_1 = (v_1, U_{nnz(x_1)})$ and $\omega_2 = (v_2, U_{nnz(x_2)})$, respectively. Note that $\omega_1$ and $\omega_2$ does not overlap because the categories and the non-zeros entries of training examples do not overlap. The computations of their partial derivatives are also independent of each other. 
Hence the update procedures in Line \ref{stp:asgd-update-begin}-\ref{stp:asgd-update-end} can happen simultaneously without the need for a lock. 
Finally we make use of momentum in the update (Line \ref{stp:asgd-v-momentum} and \ref{stp:asgd-u-momentum}). The descent direction is obtained by a linear combination of current gradient and a running average of previous gradients. This has been shown to accelerate SGD in the relevant direction and dampen oscillations especially during navigating ravines \cite{rumelhart1988learning}.

\begin{algorithm2e}[htb]
\caption{Asynchronous SGD With Momentum.}
\label{alg:asgd}
  \textbf{Input}: $\gamma \in (0, 1), \eta > 0, T > 0$, 
  the training set $\DCal \subset \XCal \times \CCal$ \\
  initialize randomly the trainable parameters $U, V$ \\
  initialize to zero the momentum parameters $\bar U, \bar V$ \\
  \For(\tcp*[h]{asynchronously}) {each thread} {
    \For(\tcp*[h]{main iteration}) {$t=1,2,...,T$} {
	    sample $(x, c_x) \in \DCal$ \\
	    $\bar v_{c_x} \gets \gamma \bar v_{c_x} + \nabla_{v_{c_x}} \JCal(U, V)$ \\
	    \label{stp:asgd-update-begin}
	    \label{stp:asgd-v-momentum}
	    $v_{c_x} \gets v_{c_x} - \eta \bar v_{c_x}$ \\
	    \For{each $i \in nnz(x)$} {
	    	$\bar u_i \gets \gamma \bar u_i + \nabla_{u_i} \JCal(U, V)$ \\
		    \label{stp:asgd-u-momentum}
	    	$u_i \gets u_i - \eta \bar u_i$
	    }
	    \label{stp:asgd-update-end}
    }
  }
  \textbf{Output}: model parameters $U, V$
\end{algorithm2e}
























