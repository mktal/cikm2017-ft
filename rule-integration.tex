% @Author: Xiaocheng Tang
% @Date:   2017-05-11 22:01:21
% @Last Modified by:   Xiaocheng Tang
% @Last Modified time: 2017-05-21 00:23:37
% !TEX root = ./abstract.tex

In this work we utilize generative models to create new training labels and de-noise the data (\cite{Ratner2016}).
The model applies generative processes to labeling functions $\lambda$ and learns their structural correlations and conflicts.
We incorporate numerous labeling functions as a supply of domain knowledge for weak supervision.

The generative model optimizes noise aware loss function and estimates probabilities $p(Label, \lambda)$,
where $Label$ is the category we want to predict, $\lambda = f(x)$ is a list of values assigned by labeling
functions $f$ to observation $x$, $\lambda : \chi \mapsto \{1, 0, -1\}^m$, where m is the number of labeling functions.
The values assigned by labeling functions are aggregated and serve as input to generative model.

Infered probabilies of the generative model are used to train another noise-aware end discriminative model which
predicts the ultimate categories.

% weak supervision, in which training labels are noisy and may be from
% multiple, potentially overlapping sources
% To address this, we model the labeling functions as a generative
% process, which lets us automatically de-noise the resulting training set by learning the accuracies of
% the labeling functions along with their correlation structure. In turn, we use this model of the training
% set to optimize a stochastic version of the loss function of the discriminative model that we desire to train.`' 