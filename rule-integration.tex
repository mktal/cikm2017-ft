% @Author: Xiaocheng Tang
% @Date:   2017-05-11 22:01:21
% @Last Modified by:   Xiaocheng Tang
% @Last Modified time: 2017-05-11 22:02:26
% !TEX root = ./abstract.tex

In this work we utilize generative models to create new training labels and de-noise the data (\cite{genmodels}). The model applies generative processes to labeling functions and learns their structural correlations and conflicts. We incorporate numerous labeling functions as a supply of domain knowledge for weak supervision. Table (\ref{tab:numrules}) lists some of the categories and the number of rules associated with them. Labeling functions assign 1 if the product belongs to the category, -1 if it does not and 0 if it is uncertain\cite{genmodels}.

The generative model optimizes noise aware loss function and estimates probabilities $p(Label, \lambda)$, where $Label$ is the category we want to predict, $\lambda = f(x)$ is a list of values assigned by labeling function $f$ to product $x$.  Since current implementation of generative model is binary we apply it for each category separately and combine resulted labeled data in a final output. Picture (\ref{fig:genmodel_pred_probs}) shows the distribution of marginal probabilities for several classes. We assume that the product belongs to certain category if prediction probability is greater or equal to 0.5.

Ultimately, we combine generated and manually labeled training sets to train a discriminative model to predict accurate product categories.

\begin{table*}
  \caption{Examples of the category labels and the total numbers of rules in each category}
  \label{tab:numrules}
  \begin{tabular}{ll}
    \toprule
    Category & \#Rules\\
    \midrule
    Boots \textgreater Women &	88	\\
    Boots \textgreater Men	& 36 \\
 	Travel \textgreater Hotel & 30 \\
 	Home \textgreater LampShades & 19 \\
 	Home \textgreater Other & 11 \\
 	Grocery \textgreater Nutrition Drinks & 11 \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}
  \caption{Evaluation results for several categories on sub-sampled data}
  \label{tab:evaluation}
  \begin{tabular}{lllllll}
    \toprule
    Category & Presicion & Recall & F1 & Accuraccy & Neg Class Accuracy & Pos Class Accuracy\\
    \midrule 
    Boots \textgreater Women	& 98\% & 100\% & 99\% & 98\% & 100\% & 98\% \\
    Boots \textgreater Men &	95\%	& 100\% & 97\% & 98\% & 100\% & 95\% \\
    \bottomrule
  \end{tabular}
\end{table*}
\begin{figure}[th]
   \includegraphics[width=0.5\textwidth]{resources/genmodel_pred_probs}
   \caption{The distribution of marginal probabilities}
   \label{fig:genmodel_pred_probs}
   \centering
\end{figure}


/*
weak supervision, in which training labels are noisy and may be from
multiple, potentially overlapping sources
To address this, we model the labeling functions as a generative
process, which lets us automatically de-noise the resulting training set by learning the accuracies of
the labeling functions along with their correlation structure. In turn, we use this model of the training
set to optimize a stochastic version of the loss function of the discriminative model that we desire to train. 
*/